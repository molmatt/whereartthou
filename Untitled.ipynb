{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "100% [..............................................................................] 30617 / 30617Found 3 images belonging to 1 classes.\n",
      "Found 1 images belonging to 1 classes.\n",
      "Found 10 images belonging to 1 classes.\n",
      "Found 4 images belonging to 1 classes.\n",
      "Found 1 images belonging to 1 classes.\n",
      "Found 10 images belonging to 1 classes.\n",
      "Found 7 images belonging to 1 classes.\n",
      "Found 10 images belonging to 1 classes.\n",
      "Found 3 images belonging to 1 classes.\n",
      "Found 2 images belonging to 1 classes.\n",
      "Found 1 images belonging to 1 classes.\n",
      "Found 8 images belonging to 1 classes.\n",
      "Found 4 images belonging to 1 classes.\n",
      "Found 10 images belonging to 1 classes.\n",
      "Found 3 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from tweepy import OAuthHandler\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.applications import VGG16\n",
    "import wget\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#NYC museum information\n",
    "NYMdf = pd.read_csv('NYMuseums.csv')\n",
    "#Reading in the trained convolutional neural network VGG16 transfer trained on\n",
    "#art pictures and not art, also setting up stuff for filtering\n",
    "vgg_conv = vgg_conv = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "vgg_conv._make_predict_function()\n",
    "ArtClassy = keras.models.load_model('ArtClass.h5')\n",
    "ArtClassy._make_predict_function()\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Twitter api information, this shouldn't get loaded to github\n",
    "#consumer_key = os.environ.get('consumer_key')\n",
    "#consumer_secret = os.environ.get('consumer_secre')\n",
    "#access_token = os.environ.get('access_token')\n",
    "#access_secret = os.environ.get('access_secret')\n",
    "\n",
    "consumer_key = 'OUqMeG9ej7AOIWZe49h7Pn0BA'\n",
    "consumer_secret = 'lmrL8SAi4rn8jDcMxDguJbstB148zLow5nP5PPsulK77jFuoGv'\n",
    "access_token = '1039211579538120704-lWwy6SVfqvFnqwmRBabA0mCvGCBjJE'\n",
    "access_secret = 'dAa9wlAR4XhdivAaFDAyqjbIbnPZkSGqQU0kNYs6dPIuo'\n",
    "\n",
    "@classmethod\n",
    "def parse(cls, api, raw):\n",
    "    status = cls.first_parse(api, raw)\n",
    "    setattr(status, 'json', json.dumps(raw))\n",
    "    return status\n",
    "\n",
    "# Status() is the data model for a tweet\n",
    "tweepy.models.Status.first_parse = tweepy.models.Status.parse\n",
    "tweepy.models.Status.parse = parse\n",
    "# User() is the data model for a user profil\n",
    "tweepy.models.User.first_parse = tweepy.models.User.parse\n",
    "tweepy.models.User.parse = parse\n",
    "# You need to do it for all the models you need\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def getart():\n",
    "    wd = os.getcwd()\n",
    "    twitartFP = os.path.join(wd,'static/twitart/')\n",
    "    NYMfp = os.path.join(wd,'NYMuseums.csv')\n",
    "    NYMdf = pd.read_csv(NYMfp)\n",
    "\n",
    "# Removing yesterdays images\n",
    "    twitterFP=[]\n",
    "    for path, subdirs, files in os.walk(twitartFP):\n",
    "        for name in files:\n",
    "            twitterFP.append(os.path.join(path, name))\n",
    "    for f in twitterFP:\n",
    "        os.remove(os.path.join(twitartFP, f))\n",
    "    twitterFP=[]\n",
    "    for path, subdirs, files in os.walk(twitartFP):\n",
    "        for name in files:\n",
    "            twitterFP.append(os.path.join(path, name))\n",
    "    print(twitterFP)\n",
    "# Getting new images\n",
    "    for i in range(0, len(NYMdf[\"Twitter\"])):\n",
    "        if str(NYMdf[\"Twitter\"][i]) != \"nan\":\n",
    "            sn = str(NYMdf[\"Twitter\"][i])\n",
    "            tweets = api.user_timeline(screen_name=sn,\n",
    "                               count=500, include_rts=False,\n",
    "                               exclude_replies=True)\n",
    "            media_files = set()\n",
    "            for status in tweets:\n",
    "                media = status.entities.get('media', [])\n",
    "                if(len(media) > 0):\n",
    "                    media_files.add(media[0]['media_url'])\n",
    "            tweets = []\n",
    "            media_files = list(media_files)\n",
    "            if len(media_files) > 10:\n",
    "                for j in range(0, 10):\n",
    "                    try:\n",
    "                        fp = twitartFP+str(NYMdf[\"Twitter\"][i])+\"/pics/\"+str(j)+\".jpg\"\n",
    "                        wget.download(media_files[j], out=fp)\n",
    "                    except OSError:\n",
    "                        continue\n",
    "            elif len(media_files) > 0:\n",
    "                for j in range(0, len(media_files)):\n",
    "                    try:\n",
    "                        fp = twitartFP+str(NYMdf[\"Twitter\"][i])+\"/pics/\"+str(j)+\".jpg\"\n",
    "                        wget.download(media_files[j], out=fp)\n",
    "                    except OSError:\n",
    "                        continue\n",
    "\n",
    "# Filtering out not art\n",
    "    nah, musfolds, nope = next(os.walk(twitartFP))\n",
    "    for g in musfolds:\n",
    "        path, dirs, files = next(os.walk(twitartFP+str(g)+\"/pics/\"))\n",
    "        nTest = len(files)\n",
    "        batch_size = 1\n",
    "        if nTest != 0:\n",
    "            test_dir = nah+str(g)\n",
    "            test_features = np.zeros(shape=(nTest, 7, 7, 512))\n",
    "            test_generator = datagen.flow_from_directory(\n",
    "                test_dir,\n",
    "                target_size=(224, 224),\n",
    "                batch_size=1,\n",
    "                class_mode=None,\n",
    "                shuffle=False)\n",
    "# Feeding through the VGG16\n",
    "            i = 0\n",
    "            for inputs_batch in test_generator:\n",
    "                features_batch = vgg_conv.predict(inputs_batch)\n",
    "                test_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "                i += 1\n",
    "                if i * batch_size >= nTest:\n",
    "                    break\n",
    "# Feeding VGG16 output through art classifying layers and filtering out 'not art'\n",
    "            test_features = np.reshape(test_features, (nTest, 7 * 7 * 512))\n",
    "            prediction = ArtClassy.predict_classes(test_features)\n",
    "            for x in range(0, len(prediction)):\n",
    "                if prediction[x] == 1:\n",
    "                    os.remove(os.path.join(path, files[x]))\n",
    "\n",
    "getart()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
